import os
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report, confusion_matrix

DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
SRC_DIR = os.path.join(os.path.dirname(__file__), "..", "src")

CSV_PATH = os.path.join(DATA_DIR, "dataset.csv")

# Strict, global feature order â€“ MUST match ESP32:
# f1,f2,f3,f4,f5,gdp,ax,ay,az,gx,gy,gz
FEATURE_COLS = [
    "f1", "f2", "f3", "f4", "f5",
    "gdp",
    "ax", "ay", "az",
    "gx", "gy", "gz",
]

LABEL_COL = "label"


def export_scaler_params(scaler: StandardScaler, out_path: str) -> None:
    # Force numpy arrays with dtype float so type-checkers are happy
    mean = np.asarray(scaler.mean_, dtype=float)
    scale = np.asarray(scaler.scale_, dtype=float)
    num_features = int(mean.shape[0])

    lines = []
    lines.append("#pragma once")
    lines.append("#include <Arduino.h>")
    lines.append("")
    lines.append(f"#define NUM_FEATURES {num_features}")
    lines.append("")
    mean_vals = ", ".join(f"{float(m):.6f}f" for m in mean)
    lines.append(f"static const float SCALER_MEAN[NUM_FEATURES] = {{ {mean_vals} }};")
    scale_vals = ", ".join(f"{float(s):.6f}f" for s in scale)
    lines.append(f"static const float SCALER_SCALE[NUM_FEATURES] = {{ {scale_vals} }};")
    lines.append("")
    lines.append("inline void standardizeFeatures(float feat[NUM_FEATURES]) {")
    lines.append("  for (int i = 0; i < NUM_FEATURES; ++i) {")
    lines.append("    feat[i] = (feat[i] - SCALER_MEAN[i]) / SCALER_SCALE[i];")
    lines.append("  }")
    lines.append("}")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote scaler params to {out_path}")


def export_label_names(le: LabelEncoder, out_path: str) -> None:
    classes = list(le.classes_)
    num = len(classes)

    lines = []
    lines.append("#pragma once")
    lines.append("")
    lines.append(f"#define NUM_CLASSES {num}")
    lines.append("")
    lines.append("static const char* const label_names[NUM_CLASSES] = {")
    for i, name in enumerate(classes):
        safe = str(name).replace("\\", "\\\\").replace('"', '\\"')
        comma = "," if i < num - 1 else ""
        lines.append(f'  "{safe}"{comma}')
    lines.append("};")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote label names to {out_path}")


def export_knn_model(
    knn: KNeighborsClassifier,
    scaler: StandardScaler,
    le: LabelEncoder,
    X_scaled: np.ndarray,
    y_enc: np.ndarray,
    out_path: str,
) -> None:
    n_samples, n_features = X_scaled.shape

    # metric / weights / n_neighbors via getattr so Pylance is happy
    metric = getattr(knn, "metric", "euclidean")
    if metric == "euclidean":
        metric_code = 0
    elif metric == "manhattan":
        metric_code = 1
    else:
        raise ValueError(f"Unsupported metric for export: {metric}")

    weights = getattr(knn, "weights", "uniform")
    if weights == "uniform":
        weights_code = 0
    elif weights == "distance":
        weights_code = 1
    else:
        raise ValueError(f"Unsupported weights for export: {weights}")

    k = int(getattr(knn, "n_neighbors", 3))

    lines = []
    lines.append("#pragma once")
    lines.append("#include <Arduino.h>")
    lines.append("#include \"scaler_params.h\"")
    lines.append("")
    lines.append("// Auto-generated by train_knn.py")
    lines.append(f"#define KNN_K        {k}")
    lines.append(f"#define KNN_METRIC   {metric_code}  // 0=L2, 1=L1")
    lines.append(f"#define KNN_WEIGHTS  {weights_code}  // 0=uniform, 1=distance")
    lines.append("")
    lines.append(f"#define NUM_SAMPLES  {n_samples}")
    lines.append("")
    lines.append("static const float X_train[NUM_SAMPLES][NUM_FEATURES] PROGMEM = {")
    for i in range(n_samples):
        row = ", ".join(f"{float(v):.6f}f" for v in X_scaled[i])
        comma = "," if i < n_samples - 1 else ""
        lines.append(f"  {{ {row} }}{comma}")
    lines.append("};")
    lines.append("")
    lines.append("static const uint8_t y_train[NUM_SAMPLES] PROGMEM = {")
    row = ", ".join(str(int(v)) for v in y_enc)
    lines.append(f"  {row}")
    lines.append("};")
    lines.append("")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"Wrote KNN model to {out_path}")


def main() -> None:
    if not os.path.exists(CSV_PATH):
        raise SystemExit(f"dataset.csv not found at {CSV_PATH}")

    df = pd.read_csv(CSV_PATH)

    missing = [c for c in FEATURE_COLS + [LABEL_COL] if c not in df.columns]
    if missing:
        raise SystemExit(f"dataset.csv missing columns: {missing}")

    # Force numpy arrays so types are clean
    X: np.ndarray = df[FEATURE_COLS].to_numpy(dtype=float)
    y_str: np.ndarray = df[LABEL_COL].astype(str).to_numpy()

    print(f"Loaded dataset: {X.shape[0]} samples, {X.shape[1]} features")

    le = LabelEncoder()
    y_enc: np.ndarray = np.asarray(le.fit_transform(y_str), dtype=int)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.2, stratify=y_enc, random_state=42
    )

    pipe = Pipeline(
        [
            ("scaler", StandardScaler()),
            ("knn", KNeighborsClassifier()),
        ]
    )

    param_grid = {
        "knn__n_neighbors": [3, 5, 7],
        "knn__metric": ["euclidean", "manhattan"],
        "knn__weights": ["uniform", "distance"],
    }

    grid = GridSearchCV(
        pipe,
        param_grid=param_grid,
        cv=5,
        n_jobs=-1,
        verbose=1,
    )
    grid.fit(X_train, y_train)

    print("\nBest params:", grid.best_params_)
    best = grid.best_estimator_

    y_pred = best.predict(X_test)
    print("\nClassification report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))
    print("Confusion matrix:")
    print(confusion_matrix(y_test, y_pred))

    scaler: StandardScaler = best.named_steps["scaler"]  # type: ignore[assignment]
    knn: KNeighborsClassifier = best.named_steps["knn"]  # type: ignore[assignment]

    os.makedirs(SRC_DIR, exist_ok=True)

    export_scaler_params(scaler, os.path.join(SRC_DIR, "scaler_params.h"))
    export_label_names(le, os.path.join(SRC_DIR, "label_names.h"))

    X_all_scaled: np.ndarray = np.asarray(scaler.transform(X), dtype=float)
    y_all_enc: np.ndarray = np.asarray(le.transform(y_str), dtype=np.uint8)

    export_knn_model(
        knn,
        scaler,
        le,
        X_all_scaled,
        y_all_enc,
        os.path.join(SRC_DIR, "glove_knn_model.h"),
    )

    print("\nAll headers exported. Rebuild the firmware in PlatformIO.")

if __name__ == "__main__":
    main()
